# Texture
## Shape and Texture

![[Computer Vision/Images/shape_and_texture.png]]

## Learning Outcomes
- Explain parametric and non-parametric methods for representing and synthesising texture
- Explain common applications of texture synthesis
- Explain an algorithm for texture transfer (Neural Style Transfer)

# What is a Texture?
- A definition from image processing:
	Texture is a region with **spatial stationary** (same statistical properties everywhere in the region)
- A definition from computer graphics:
	Texture is 2D surface applied to a 3D model

## Types of Texture
- Periodic texture - has a subregion that repeats in a regular pattern

![[periodic_texture.png]]

- Stochastic (aperiodic) texture - generated by a random process

![[aperiodic_texture.png]]

## Texture Models
- **Parametric** models: represent texture with a set of adjustable parameters

![[parametric_texture.png]]

- **Non-parametric** (stitching) models: represent texture as image patches

![[nonparametric_texture.png]]

## Why Model Texture?
- Texture synthesis - create more of a texure
	- Textures for computer graphics, video games, etc.
	- Image inpainting
- Texture transfer
	- Artistic effects
	- Online shopping

# Texture Synthesis
## Image Stitching Approach
- How do you fill in the missing data?
	- Look at a neighbourhood around this patch...

![[texture_synthesis.png]]

- And find similar neighbourhoods in other parts of the texture

![[similar_neighbourhood_pattersn.png]]

Original patch:

![[original_patch.png]]

Similar patches:

![[neighbourhood_of_Cracks.png]]

Select probable value for the missing pixel, based on similar patches:

![[most_probable_patch.png]]

## Non-Parametric Texture Synthesis
1. Randomly sample a small (e.g., 3x3 pixel) patch from the original image
2. Spiral outward, filling in missing pixels by finding similar neighbourhoods in the original texture
(Neighbourhood size is a free parameter that specifies how stochastic the texture is)

![[non_parametric_texture_synthesis.png]]

## Neighbourhood Size
- As we can see with the performance of different neighbourhood sizes below, the performance really depends on the actual texture and the size of the neighbourhood for that texture
- As shown below, too small a neighbourhood and the texture is completely different, bigger neighbourhoods in this case give a better result.

![[neighbourhood_size.png]]

## Image Quilting
- Efficient patch-based texture synthesis 
- Use existing patches of texture to synthesise more texture; main problem is connecting them together without visible artefacts/seams
- "Corrupt Professor's Algorithm"
	- Plagiarise as much of the source image as you can
	- Then try to cover up the evidence

## Image Quilting Algorithm
- Choose patch and overlap size
- Initialise with a random patch
- For each subsequent patch:
	- Find a patch in the original texture that is most similar to this region, considering only the pixels in the overlap region
	- Seamlessly paste in patch by cutting along a path with minimum overlap error

![[overlapping.png]]

- An example is shown below in action:

![](https://upload.wikimedia.org/wikipedia/commons/b/bc/Imagequilting.gif)

- The way we can hide this overlapping area being so visible as shown on the right-hand image is by: 
	- Finding a cut along the overlap with provides the minimum error from to top to bottom of the image
	 - Cutting from the first part and then the second part and then sewing the two parts together, as shown in the last picture

![[image_quilting_process.png]]

## Graph Cuts
- Represent neighbouring pixels as a graph
- Edge weight = overlap error
- Problem: Find path through graph with minimum total overlap error
- Just like the seam carver!

![[graph_cuts_for_patching.png]]

## Image Quilting Results
- As shown below, the results are not bad

![[image_quilting_results.png]]

- However, as shown below, for text it doesn't really do well at capturing the structure and looks a bit off on closer look

![[image_quilting_results2.png]]

![[quilting_results3.png]]

- Same goes for the image below

![[seagulls.png]]

![[seagulls_patched.png]]

## Image Inpainting
- Similar idea to fill in missing regions of an image:
	- Find a similar patch in _another_ image
	- Paste in patch with an error-minimising cut
- Also notice the images on the side, we will be taking cuts from these images for the one that is the most similar and blend it in this patch.

![[image_inpainting.png]]

## Parametric Texture Synthesis
- Alternative to stitching approaches: represent texture with a number of parameters
- To synthesise texture, coerce a noise image to match the required parameters (usually through gradient descent)
- What parameters are needed to define a texture?

### Fourier Magnitude?
- We can use the Fourier transform to get parameters for texture
- Below shows that by keeping the phase and randomising the magnitude, we actually can still keep the structure of the texture for the image. However, we it doesn't really keep the actual appearance of the image. 
- While when we keep the magnitude and randomise the the phase, it doesn't really seem to keep the texture as well.

![[fourier_magnitude_parametric.png]]

## Fourier Texture Synthesis
- Synthesise texture by matching Fourier magnitude
- Okay results for some simple textures, but doesn't work well in general

![[fourier_texture_synthesis.png]]

## Colours and Edges?
- Textures could be defines as a distribution over simple features, like colour and edge orientation at various scales
- Synthesise texture by matching the distribution

![[colours_and_edges.png]]

## Distribution-Matching Results

![[distribution_matching_results.png]]

## More Complex Statistics?
- Simple distributions of features are not sufficient
- Also need to represent feature co-occurrence
- For structured images it doesn't perform very well

![[more_complex_solutions.png]]

## Texture Synthesis Results
- Texture synthesis does okay on simple textures, but high level, more complex textures it performs poorly.

![[texture_synthesis_reults.png]]

## Even More Complex Statistics?
- The set of statistics needed to represent real images may be very complex
- Instead of modelling statistics by hand, represent texture as the feature response in the layers of a neural network trained on ImageNet classification
- As can be seen in the images below, in the first layer it is mainly focused on local features. But as we continue through the layers of the network the feature responses slowly start to take the shape of the texture that we are looking for.

![[even_more_complex_statistics.png]]

## Feature Correlations
- Texture is represented as the correlations between feature maps at a layer of the neural network:

![[feature_correlations.png]]

- In this image, $l$ means the layer number, $i$ and $j$ mean the feature map number from layer number $l$, and $k$ means the pixel number of the feature maps.
- $G^l_{ij}$ is the covariance matrix for the feature responses of feature maps $i$ and $j$ of layer $l$. We use this to generate our textures.

## Textures Synthesis Results
- As the results show below, deeper into the network we get some pretty nice results, except for really structured images such as the one on the right.

![[textres_synthesis_results.png]]

- Much the same for these images

![[texture_synthesis_results2.png]]

## Summary
- Non-parametric texture synthesis is based on copying texture patches
	- Works very well on periodic textures
	- Disadvantage: No model of texture parameters
- Parametric textures work better on stochastic textures
	- Most methods work better on stochastic textures
	- Disadvantage: Even very complex models (e.g., based on neural networks) may be incomplete

# Texture Transfer
- Render an image in the style of another image:

![[texture_transfer.png]]

## Neural Style Transfer Algorithm
- Both images (content, style) are run through a VGG network trained on ImageNet
- Content is represented as the response from a layer of the neural network
- Style is represented as the correlations between feature maps at a layer of the network
- Use gradient descent to find an image that matches both style and content

## Neural Style Transfer
- We feed the images into a neural net as shown below, and we pull out the feature maps at each highlighted red point and reconstruct the images with separate neural networks. 
- We first feed in the image that holds the structure. As can be seen with the quality of the house as we progress deeper into the network, the quality of the image degrades. This is because of the max pooling layers reducing the spatial precision of the images.
- We then feed in the style image through the network and find the correlations between the feature maps as we did for feature correlations technique previously and then feeding these into neural nets for reconstruction. As can be seen with the starry night image, the image focuses mainly on local features at the beginning and then focuses on more global features deeper into the network.
- You then pick a reconstructed image of each and then feed in some white noise to basically make the neural network form a blend of the two images, thus creating our neural style transfer image!

![[neural_style_transfer.png]]

## Style Transfer Parameters
- Loss is sum of loss from content reconstruction and style reconstruction
- Relative weight of content vs. style is a free parameter:

![[Computer Vision/Images/style_transfer_parameters.png]]

- Content and style can be matched at any combination of layers
- Generally, match content at higher layers, and style across all layers

![[style_image.png]]

![[style_transfer_parameters 1.png]]

## Texture Transfer Results

![[texture_transfer_results.png]]

## Summary
- Texture transfer - render an image in the style of another image
- Image content represented by neural network responses
- Texture content represented by neural network responses
- Texture represented by correlations of feature maps across multiple layers of a neural network